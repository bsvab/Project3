{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the new DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a new dataset in BigQuery\n",
    "def create_dataset(project_id, dataset_id):\n",
    "    # Set the environment variable to point to your service account key file\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key-file.json\"\n",
    "\n",
    "    # Initialize the BigQuery client\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Construct a full Dataset object to send to the API\n",
    "    dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "\n",
    "    # Specify the geographic location where the dataset should reside\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    try:\n",
    "        # Send the dataset to the API for creation\n",
    "        dataset = client.create_dataset(dataset, timeout=30)\n",
    "        print(f\"Dataset {dataset.dataset_id} created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for the project id and the new dataset id\n",
    "project_id = \"project-3-415202\"\n",
    "dataset_id = \"seismic_activity_and_injection_wells\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset seismic_activity_and_injection_wells created.\n"
     ]
    }
   ],
   "source": [
    "# Pass the function to create the data set in Big Query\n",
    "create_dataset(project_id, dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating tables in the DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to create the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a new table in BigQuery\n",
    "def create_table(project_id, dataset_id, table_id, schema):\n",
    "    # Set the environment variable to point to your service account key file\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key-file.json\"\n",
    "\n",
    "    # Initialize the BigQuery client\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Define the dataset reference\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "    # Define the table schema\n",
    "    table_schema = []\n",
    "    for field in schema:\n",
    "        table_schema.append(bigquery.SchemaField(field['name'], field['type']))\n",
    "\n",
    "    # Define the table reference\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    # Define the table object\n",
    "    table = bigquery.Table(table_ref, schema=table_schema)\n",
    "\n",
    "    try:\n",
    "        # Send the table to the API for creation\n",
    "        table = client.create_table(table) \n",
    "        print(f\"Table {table_id} created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the earthquakes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for the earthquakes table\n",
    "project_id = \"project-3-415202\"\n",
    "dataset_id = \"seismic_activity_and_injection_wells\"\n",
    "table_id = \"earthquakes\"\n",
    "schema = [\n",
    "    {\"name\": \"Latitude\", \"type\": \"NUMERIC\"},\n",
    "    {\"name\": \"Longitude\", \"type\": \"NUMERIC\"},\n",
    "    {\"name\": \"Magnitude\", \"type\": \"NUMERIC\"},\n",
    "    {\"name\": \"Event_Date\", \"type\": \"DATE\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table earthquakes created.\n"
     ]
    }
   ],
   "source": [
    "# Create the table\n",
    "create_table(project_id, dataset_id, table_id, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the injectionVolumes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for the injectionVolumes table\n",
    "project_id = \"project-3-415202\"\n",
    "dataset_id = \"seismic_activity_and_injection_wells\"\n",
    "table_id = \"injectionVolumes\"\n",
    "schema = [\n",
    "    {\"name\": \"API Number\", \"type\": \"INTEGER\"},\n",
    "    {\"name\": \"Surface Longitude\", \"type\": \"NUMERIC\"},\n",
    "    {\"name\": \"Surface Latitude\", \"type\": \"NUMERIC\"},\n",
    "    {\"name\": \"Injection Date\", \"type\": \"DATE\"},\n",
    "    {\"name\": \"Injection End Date\", \"type\": \"DATE\"},\n",
    "    {\"name\": \"Volume Injected: BBLs\", \"type\": \"NUMERIC\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table injectionVolumes created.\n"
     ]
    }
   ],
   "source": [
    "# Create the table\n",
    "create_table(project_id, dataset_id, table_id, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pressureData table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for the injectionVolumes table\n",
    "project_id = \"project-3-415202\"\n",
    "dataset_id = \"seismic_activity_and_injection_wells\"\n",
    "table_id = \"pressureData\"\n",
    "schema = [\n",
    "    {\"name\": \"Time\", \"type\": \"DATE\"},\n",
    "    {\"name\": \"Pressure\", \"type\": \"NUMERIC\"},\n",
    "    {\"name\": \"Layer\", \"type\": \"STRING\"},\n",
    "    {\"name\": \"Longitude\", \"type\": \"NUMERIC\"},\n",
    "    {\"name\": \"Latitude\", \"type\": \"NUMERIC\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table pressureData created.\n"
     ]
    }
   ],
   "source": [
    "# Create the table\n",
    "create_table(project_id, dataset_id, table_id, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading data into the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_bigquery(csv_file_path, project_id, dataset_id, table_id, schema=None):\n",
    "    # Set the environment variable to point to your service account key file\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key-file.json\"\n",
    "\n",
    "    # Load CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Define table reference\n",
    "    table_ref = f\"{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Write DataFrame to BigQuery\n",
    "    df.to_gbq(destination_table=table_ref,\n",
    "              project_id=project_id,\n",
    "              if_exists='replace',  # Replace table if it already exists\n",
    "              table_schema=schema)  # Optional explicit schema\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' successfully loaded into BigQuery table '{table_ref}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the earthquakes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for the table\n",
    "csv_file_path = os.path.join('..', 'earthquake-data', 'earthquakes.csv')\n",
    "project_id = 'project-3-415202'\n",
    "dataset_id = 'seismic_activity_and_injection_wells'\n",
    "table_id = 'earthquakes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_192\\2202039736.py:12: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(destination_table=table_ref,\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '..\\earthquake-data\\earthquakes.csv' successfully loaded into BigQuery table 'seismic_activity_and_injection_wells.earthquakes'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "load_csv_to_bigquery(csv_file_path, project_id, dataset_id, table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the injectionVolumes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for the table\n",
    "csv_file_path = os.path.join('..', 'injectionVolumes-data', 'injectionVolumes.csv')\n",
    "project_id = 'project-3-415202'\n",
    "dataset_id = 'seismic_activity_and_injection_wells'\n",
    "table_id = 'injectionVolumes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_18392\\1369449875.py:12: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(destination_table=table_ref,\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '..\\injectionVolumes-data\\injectionVolumes.csv' successfully loaded into BigQuery table 'seismic_activity_and_injection_wells.injectionVolumes'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "load_csv_to_bigquery(csv_file_path, project_id, dataset_id, table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Pressure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for the table\n",
    "csv_file_path = os.path.join('..', 'pressure-data', 'Updated_Pressure_Data_with_LatLon.csv')\n",
    "project_id = 'project-3-415202'\n",
    "dataset_id = 'seismic_activity_and_injection_wells'\n",
    "table_id = 'pressureData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_18392\\1369449875.py:12: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(destination_table=table_ref,\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '..\\pressure-data\\Updated_Pressure_Data_with_LatLon.csv' successfully loaded into BigQuery table 'seismic_activity_and_injection_wells.pressureData'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "load_csv_to_bigquery(csv_file_path, project_id, dataset_id, table_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
